{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc334c-92c6-4ddd-ba63-bbe2cca3c9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymongo\n",
    "!pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbae927e-ad41-4f48-a177-7d4c932fadc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post inserido com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "\n",
    "# Conecta-se ao MongoDB\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['forum_database']  # Banco de dados que as coletas serão armazenadas\n",
    "posts_collection = db['posts']  # Coleção dos posts coletados\n",
    "\n",
    "# Função para fazer o scraping e pegar os dados do site\n",
    "def coletar_dados(url):\n",
    "    \"\"\"\n",
    "    Coleta os dados do site e retorna as informações. Como será utilizado o site Stack Overflow, a estrutura de um post é:\n",
    "    Título do post\n",
    "    Conteúdo/Dúvida\n",
    "    Respostas\n",
    "    Comentários\n",
    "    \"\"\"\n",
    "    # Faz a requisição à página\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Se a requisição for bem-sucedida, procede com o scraping\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Coleta as informações do post\n",
    "        titulo_post = soup.find('a', class_='question-hyperlink').text.strip()\n",
    "        \n",
    "        # Altera para encontrar o conteúdo corretamente\n",
    "        conteudo_post = soup.find('div', class_='post-text')\n",
    "        if conteudo_post:\n",
    "            conteudo_post = conteudo_post.text.strip()\n",
    "        else:\n",
    "            conteudo_post = \"Conteúdo não disponível\"\n",
    "        \n",
    "        usuario = soup.find('div', class_='user-details').a.text.strip()\n",
    "        \n",
    "        # Tentando encontrar a data do post original\n",
    "        data_post_tag = soup.find('span', class_='relativetime')\n",
    "        if data_post_tag:\n",
    "            data_post_str = data_post_tag['title']  # A data está no atributo 'title'\n",
    "            data_post = datetime.strptime(data_post_str, '%Y-%m-%d %H:%M:%SZ')  # Novo formato ISO 8601\n",
    "        else:\n",
    "            data_post = datetime.now()  # Se não encontrar, utiliza a hora atual\n",
    "        \n",
    "        url_post = url  # URL do post\n",
    "\n",
    "        # Coletando as respostas\n",
    "        respostas = []\n",
    "        resposta_divs = soup.find_all('div', class_='answer')\n",
    "        \n",
    "        for resposta in resposta_divs:\n",
    "            # Coleta o autor da resposta\n",
    "            autor_resposta = resposta.find('div', class_='user-details').a.text.strip()\n",
    "            \n",
    "            # Alterando para pegar o conteúdo da resposta com a classe 's-prose'\n",
    "            conteudo_resposta = resposta.find('div', class_='s-prose')\n",
    "            if conteudo_resposta:\n",
    "                conteudo_resposta = conteudo_resposta.text.strip()\n",
    "            else:\n",
    "                conteudo_resposta = \"Conteúdo da resposta não disponível\"\n",
    "            \n",
    "            # Coletando a data da resposta\n",
    "            data_resposta_tag = resposta.find('span', class_='relativetime')\n",
    "            if data_resposta_tag:\n",
    "                data_resposta_str = data_resposta_tag['title']\n",
    "                data_resposta = datetime.strptime(data_resposta_str, '%Y-%m-%d %H:%M:%SZ')\n",
    "            else:\n",
    "                data_resposta = datetime.now()  # Se não encontrar, utiliza a hora atual\n",
    "\n",
    "            # Coleta os comentários da resposta\n",
    "            comentarios_resposta = []\n",
    "            comentario_divs = resposta.find_all('div', class_='comment')\n",
    "            \n",
    "            for comentario in comentario_divs:\n",
    "                autor_comentario = comentario.find('a', class_='comment-user').text.strip()\n",
    "                conteudo_comentario = comentario.find('span', class_='comment-copy').text.strip()\n",
    "                data_comentario = datetime.now()\n",
    "                quantidade_reacoes = len(comentario.find_all('span', class_='vote-count-post'))  # Contar reações\n",
    "                \n",
    "                comentarios_resposta.append({\n",
    "                    'autor_comentario': autor_comentario,\n",
    "                    'data_comentario': data_comentario,\n",
    "                    'conteudo_comentario': conteudo_comentario,\n",
    "                    'quantidade_reacoes': quantidade_reacoes\n",
    "                })\n",
    "            \n",
    "            respostas.append({\n",
    "                'autor_resposta': autor_resposta,\n",
    "                'conteudo_resposta': conteudo_resposta,\n",
    "                'data_resposta': data_resposta,\n",
    "                'comentarios': comentarios_resposta\n",
    "            })\n",
    "\n",
    "        # Retorna os dados que foram coletados\n",
    "        return {\n",
    "            'titulo_post': titulo_post,\n",
    "            'conteudo_post': conteudo_post,\n",
    "            'usuario': usuario,\n",
    "            'data_post': data_post,\n",
    "            'url_post': url_post,\n",
    "            'respostas': respostas\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Erro ao acessar a página: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Função que insere os dados no banco de dados\n",
    "def inserir_post(data_coleta, url_post, data_post, usuario, titulo_post, conteudo_post, respostas):\n",
    "    post = {\n",
    "        \"data_coleta\": data_coleta,\n",
    "        \"url_post\": url_post,\n",
    "        \"data_post\": data_post,\n",
    "        \"usuario\": usuario,\n",
    "        \"titulo_post\": titulo_post,\n",
    "        \"conteudo_post\": conteudo_post,\n",
    "        \"respostas\": respostas\n",
    "    }\n",
    "    result = posts_collection.insert_one(post)\n",
    "    return result.inserted_id\n",
    "\n",
    "# URL do site que será realizado a coleta de dados\n",
    "url_post = \"https://stackoverflow.com/questions/3701054/test-code-coverage-without-source-code\"\n",
    "\n",
    "# Coleta os dados do site\n",
    "dados_coletados = coletar_dados(url_post)\n",
    "\n",
    "# Se os dados foram coletados com sucesso, devem ser inseridos no banco de dados\n",
    "if dados_coletados:\n",
    "    data_coleta = datetime.now()  # Hora atual da coleta\n",
    "    inserir_post(data_coleta, dados_coletados['url_post'], dados_coletados['data_post'],\n",
    "                 dados_coletados['usuario'], dados_coletados['titulo_post'],\n",
    "                 dados_coletados['conteudo_post'], dados_coletados['respostas'])\n",
    "    print(\"Post inserido com sucesso!\")  # Confirmação positiva\n",
    "else:\n",
    "    print(\"Falha na coleta de dados.\")  # Confirmação negativa (provavelmente vai dar erro ao invés de chegar aqui)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ba5cb-95f0-4bb7-a7b3-bffa42277ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
